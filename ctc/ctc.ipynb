{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Connectionist temporal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem\n",
    "\n",
    "We have a dataset of audio clips and corresponding transcripts. <br>\n",
    "We don’t know how the characters in the transcript align to the audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Speech recognition\n",
    "\n",
    "<img src=\"./data/speech_recognition.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Handwritting recognition\n",
    "\n",
    "<img src=\"./data/handwriting_recognition.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alignment\n",
    "\n",
    "CTC works by summing over the probability of all possible alignments between an input and an output. Assume the input has length six and $Y = [c, a, t]$. One way to align $X$ and $Y$ is to assign an output character to each input step and collapse repeats.\n",
    "\n",
    "<img src=\"./data/naive_alignment.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problems of this approach: <br>\n",
    "1) it doesn’t make sense to force every input step to align to some output: for example, silence in speech recognition; <br>\n",
    "2) collapsing repeats will produce “helo” instead of “hello”. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get around these problems, CTC introduces a blank token to the set of allowed outputs.\n",
    "\n",
    "<img src=\"./data/ctc_alignment_steps.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s go back to the output $[c, a, t]$ with an input of length six. Here are a few more examples of valid and invalid alignments.\n",
    "\n",
    "<img src=\"./data/valid_invalid_alignments.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Network Outputs to Labellings\n",
    "\n",
    "For an input sequence $x$ of length $T$, define a recurrent neural network $\\mathcal{N}_w$. Let $y = \\mathcal{N}_w(x)$ be the sequence of network outputs, and denote by $y_k^t$ the activation of output unit $k$ at time t. Then $y_k^t$ is interpreted as the probability of observing label k at time t, which defines a distribution over the alphabet $L'^T$ of length T sequences over the alphabet $L' = L \\cup \\{blank\\}$:\n",
    "\n",
    "$$ p(\\pi | x) = \\prod_{t = 1}^T y_{\\pi_t}^t, \\forall \\pi \\in L'^T $$\n",
    "\n",
    "We refer to the elements of $L'^T$ as paths, and denote them $\\pi$.<br><br>\n",
    "The network outputs at different times are **conditionally independent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define a map $\\mathcal{B} : L'^T \\to L'^{\\le T}$. This map simply removes all blanks and repeated labels from the paths (e.g. $\\mathcal{B}(a−ab−) = \\mathcal{B}(−aa−−abb) = aab$). Finally define a the conditional probability of a given labelling $l \\in L'^{\\le T}$:\n",
    "\n",
    "$$ p(l | x) = \\sum_{\\pi \\in \\mathcal{B}^{−1}(l)} p(\\pi | x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./data/full_collapse_from_audio.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constructing the Classifier\n",
    "\n",
    "The output of the classifier should be the most probable labelling for the input sequence:\n",
    "\n",
    "\n",
    "$$ h(x) = \\underset{l \\in L^{\\le T}}{\\operatorname{argmax}} p(l|x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One method (best path decoding) is based on the assumption that the most probable path will corre spond to the most probable labelling:\n",
    "\n",
    "$$ h(x) \\approx \\mathcal{B}(\\pi^∗), \\quad \\textrm{where} \\quad π^∗ = \\underset{\\pi}{\\operatorname{argmax}} p(\\pi | x) $$\n",
    "\n",
    "Best path decoding is trivial to compute, however it is not guaranteed to find the most probable labelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The CTC Forward-Backward Algorithm\n",
    "\n",
    "We require an efficient way of calculating the conditional probabilities $p(l | x)$ of individual labellings. There is a problem: very many of paths corresponding to a given labelling. Fortunately the problem can be solved with a dynamic programming algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For some sequence $q$ of length $r$, denote by $q_{1:p}$ and $q_{r−p:r}$ its first and last p symbols respectively. Then for a labelling $l$, define the forward variable $\\alpha_t(s)$ to be the total probability of $l_{1:s}$ at time $t$:\n",
    "\n",
    "$$ \\alpha_t(s) \\overset{def}{\\operatorname{=}} \\sum_{\\pi : \\mathcal{B}(\\pi_{1:t}) = l_{1:s}} \\prod_{t' = 1}^t y_{\\pi_{t'}}^{t'} $$\n",
    "\n",
    "\n",
    "As we will see, $\\alpha_t(s)$ can be calculated recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To allow for blanks in the output paths, we consider a modified label sequence $l'$, with blanks added to the beginning and the end and inserted between every pair of labels:\n",
    "\n",
    "$$ l' = (b, l_1, b, l_2, \\dots, l_k, b), \\quad \\textrm{where} \\quad k = |l|, \\quad b - \\textrm{blank symbol}$$\n",
    "\n",
    "We allow all transitions between blank and non-blank labels, and also those between any pair of distinct non-blank labels. We allow all prefixes to start with either $b$ or $l_1$.\n",
    "\n",
    "<img src=\"./data/ctc_cost.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we have the following rules for initialisation\n",
    "\n",
    "$$ \\alpha_1(1) = y_b^1 $$\n",
    "$$ \\alpha_1(2) = y_{l_1}^1 $$\n",
    "$$ \\alpha_1(s) = 0, \\forall s > 2 $$\n",
    "\n",
    "And now we need to recursively define $\\alpha_{t}(s)$. There is to cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In first case, we can’t jump over $l'_{s-1}$, the previous token. The first reason is that the previous token can be an element of $L$. The second reason is that we must have a blank between repeat characters from $L$.\n",
    "\n",
    "<img src=\"./data/cost_no_skip.svg\" width=\"200\">\n",
    "\n",
    "$$ \\alpha_t(s) = (\\alpha_{t − 1}(s) + \\alpha_{t − 1}(s - 1)) y_{l'_s}^t \\quad \\textrm{if} \\quad l'_s = b \\lor l'_s = l'_{s - 2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In second case, we allowed to skip the previous token.\n",
    "\n",
    "<img src=\"./data/cost_regular.svg\" width=\"200\">\n",
    "\n",
    "$$ \\alpha_t(s) = (\\alpha_{t − 1}(s) + \\alpha_{t − 1}(s - 1) + \\alpha_{t − 1}(s - 2)) y_{l'_s}^t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The probability of $l$ is then the sum of the total probabilities of $l'$ with and without the final blank at time $T$:\n",
    "\n",
    "$$ p(l | x) = \\alpha_T(|l'|) + \\alpha_T(|l'| − 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly define backward variables $\\beta_t(s)$:\n",
    "\n",
    "$$ \\beta_t(s) \\overset{def}{\\operatorname{=}} \\sum_{\\pi : \\mathcal{B}(\\pi_{t:T}) = l_{s:|l|}} \\prod_{t' = t}^T y_{\\pi_{t'}}^{t'} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Training\n",
    "\n",
    "The aim of maximum likelihood training is to simultaneously maximise the log probabilities of all the correct classifications in the training set. In our case, this means minimising the following objective function:\n",
    "\n",
    "$$ \\mathcal{O}^{ML}(S, \\mathcal{N}_w) = - \\sum_{(x, z) \\in S} ln(p(z | x)) $$\n",
    "\n",
    "To train the network with gradient descent, we need to differentiate $\\mathcal{O}$ with respect to the network outputs:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{O}^{ML}(\\{(x, z)\\}, \\mathcal{N}_w)}{\\partial y_k^t} = - \\frac{\\partial ln(p(z | x))}{\\partial y_k^t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key point is that, for a labelling $l$, the product of the forward and backward variables at a given $s$ and\n",
    "$t$ is the probability of all the paths corresponding to $l$ that go through the symbol $s$ at time $t$:\n",
    "\n",
    "$$ \\alpha_t(s) \\beta_t(s) = \\sum_{\\pi \\in \\mathcal{B}^{-1}(l) : \\pi_t = l_s} y_{l_s}^t \\prod_{t' = 1}^T y_{\\pi_t'}^{t'} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\frac{\\alpha_t(s) \\beta_t(s)}{y_{l_s}^t} = \\sum_{\\pi \\in \\mathcal{B}^{-1}(l) : \\pi_t = l_s} p(\\pi | x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can sum over all $s$ and $t$:\n",
    "\n",
    "$$ p(l | x) = \\sum_{t = 1}^T \\sum_{s = 1}^{|l|} \\frac{\\alpha_t(s) \\beta_t(s)}{y_{l_s}^t} $$\n",
    "\n",
    "Because the network outputs are conditionally independent, we need only consider the paths going through label $k$ at time $t$ to get partial derivatives. Same label may be repeated several times, so we define the set of positions where $k$ occurs as $lab(l, k) = \\{ s : l_s = k \\}$:\n",
    "\n",
    "$$ \\frac{\\partial p(l | x)}{\\partial y_{l_s}^t} = - \\frac{1}{(y_{l_s}^t)^2} \\sum_{s \\in lab(l, k) = 1} \\alpha_t(s) \\beta_t(s) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial ln(p(l | x))}{\\partial y_{l_s}^t} = \\frac{1}{p(l | x)} \\frac{\\partial p(l | x)}{\\partial y_{l_s}^t} $$\n",
    "\n",
    "so we can compute $ \\mathcal{O}^{ML}(S, \\mathcal{N}_w) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice, the above recursions will soon lead to underflows on any digital computer. Way of avoiding this with rescaling the forward and backward variables described in original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using CTC loss in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import IPython\n",
    "\n",
    "def imshow(img):\n",
    "    _,ret = cv2.imencode('.jpg', img) \n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)\n",
    "\n",
    "def generate_sample(max_symbols, alphabet):\n",
    "    sym_h, sym_w = 20, 20\n",
    "    alphabet = list(alphabet)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    image = np.zeros((sym_h, sym_w * max_symbols), dtype=np.uint8)\n",
    "    sequence = [np.random.choice(alphabet) for _ in range(np.random.randint(max_symbols))]\n",
    "    sequence_prefix = [' ' for _ in range(np.random.randint(max_symbols - len(sequence)))]\n",
    "    sequence_postfix = [' ' for _ in range(max_symbols - len(sequence) - len(sequence_prefix))]\n",
    "    sequence_str = ''.join(sequence_prefix + sequence + sequence_postfix)\n",
    "\n",
    "    sequence = np.array(\n",
    "        [alphabet.index(c) for c in sequence] +\n",
    "        (max_symbols - len(sequence)) * [len(alphabet)])\n",
    "    cv2.putText(image, sequence_str, (0, sym_h), font, 0.8, (255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAUAMgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AP5/6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK//9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10 10 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "# model constant\n",
    "max_sequence_length = 10\n",
    "alphabet = '0123456789'\n",
    "image_size = 20, 20 * max_sequence_length\n",
    "\n",
    "image, sequence = generate_sample(max_sequence_length, alphabet)\n",
    "imshow(image)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lets define a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "inputs = tf.placeholder(shape=(None, image_size[0], image_size[1], 1), dtype=tf.float32)\n",
    "targets = tf.placeholder(shape=(None, max_sequence_length), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d_2/MaxPool:0' shape=(?, 3, 25, 64) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_block(inp, filters):\n",
    "    x = tf.layers.conv2d(inputs=inp, filters=filters,\n",
    "                         kernel_size=3, padding='same',\n",
    "                         activation='relu', kernel_initializer='he_normal')\n",
    "    x = tf.layers.dropout(inputs=x, rate=0.5)\n",
    "    x = tf.layers.max_pooling2d(inputs=x, pool_size=2,\n",
    "                                strides=2, padding='same')\n",
    "    return x\n",
    "\n",
    "x = conv_block(inputs, 16)\n",
    "x = conv_block(x, 32)\n",
    "x = conv_block(x, 64)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 25, 192) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fh, fw, fc = x.get_shape()[1:]\n",
    "\n",
    "x = tf.transpose(x, (0, 2, 1, 3))\n",
    "x = tf.reshape(x, [-1, fw, fh * fc])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack:0' shape=(?, 25, 64) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.rnn as rnn\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "cell_fw = tf.nn.rnn_cell.GRUCell(\n",
    "    num_units=32,\n",
    "    activation=tf.nn.tanh,\n",
    "    name='fw')\n",
    "cell_bw = tf.nn.rnn_cell.GRUCell(\n",
    "    num_units=32,\n",
    "    activation=tf.nn.tanh,\n",
    "    name='bw')\n",
    "\n",
    "x_batch_size = array_ops.shape(x)[0]\n",
    "\n",
    "x = tf.unstack(x, axis=1)\n",
    "outputs = rnn.static_bidirectional_rnn(\n",
    "    cell_fw=cell_fw, cell_bw=cell_bw,\n",
    "    initial_state_fw=cell_fw.zero_state(x_batch_size, dtype=tf.float32),\n",
    "    initial_state_bw=cell_bw.zero_state(x_batch_size, dtype=tf.float32),\n",
    "    inputs=x)[0]\n",
    "x = tf.stack(outputs, axis=1)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_1:0' shape=(25, ?, 11) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos = len(alphabet)\n",
    "\n",
    "logits = tf.layers.dense(inputs=x,\n",
    "                         units=len(alphabet) + 1,\n",
    "                         kernel_initializer='he_normal')\n",
    "\n",
    "target_length = tf.count_nonzero(tf.not_equal(targets, eos), axis=1, dtype=tf.int32)\n",
    "logit_length = tf.tile(\n",
    "    input=[array_ops.shape(logits)[1]],\n",
    "    multiples=[array_ops.shape(logits)[0]])\n",
    "logits = tf.transpose(logits, perm=(1, 0, 2))\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "[sparse_decoded], _ = tf.nn.ctc_greedy_decoder(\n",
    "    inputs=logits,\n",
    "    sequence_length=logit_length)\n",
    "model_preds = tf.sparse.to_dense(sp_input=sparse_decoded,\n",
    "                                 default_value=eos)\n",
    "\n",
    "# loss\n",
    "ctc_loss = tf.nn.ctc_loss_v2(labels=targets,\n",
    "                             logits=logits,\n",
    "                             label_length=target_length,\n",
    "                             logit_length=logit_length,\n",
    "                             blank_index=eos)\n",
    "model_loss = tf.reduce_mean(ctc_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now we can train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 20, 200, 1), (32, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_batch(batch_size):\n",
    "    h, w = image_size\n",
    "    batch_images = np.zeros(\n",
    "        (batch_size, h, w, 1), dtype=np.uint8)\n",
    "    batch_sequences = np.zeros(\n",
    "        (batch_size, max_sequence_length), dtype=np.uint8)\n",
    "    for idx in range(batch_size):\n",
    "        image, sequence = generate_sample(\n",
    "            max_sequence_length, alphabet)\n",
    "        image = cv2.resize(image, (w, h))\n",
    "        batch_images[idx] = np.expand_dims(image, axis=2)\n",
    "        batch_sequences[idx] = sequence\n",
    "    return batch_images, batch_sequences\n",
    "\n",
    "images, sequences = generate_batch(32)\n",
    "images.shape, sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_op = tf.contrib.layers.optimize_loss(\n",
    "    model_loss, tf.train.get_global_step(),\n",
    "    optimizer='Adam', learning_rate=0.001)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 10.561681747436523\n",
      "Predicted: []\n",
      "Sequence:  [ 1  8  4  4  4  4  6 10 10 10]\n",
      "Epoch 2 loss: 12.851635932922363\n",
      "Predicted: []\n",
      "Sequence:  [ 9  8  6  9  3  5  0 10 10 10]\n",
      "Epoch 3 loss: 2.1188340187072754\n",
      "Predicted: [ 5  1  8 10 10 10 10 10 10]\n",
      "Sequence:  [ 5  1  8 10 10 10 10 10 10 10]\n",
      "Epoch 4 loss: 0.6150761842727661\n",
      "Predicted: [ 5 10 10 10 10 10 10 10 10]\n",
      "Sequence:  [ 5 10 10 10 10 10 10 10 10 10]\n",
      "Epoch 5 loss: 0.4112735986709595\n",
      "Predicted: [8 0 9 0 7 6 2 2 3]\n",
      "Sequence:  [ 8  0  9  0  7  6  2  2  3 10]\n",
      "Epoch 6 loss: 0.19175752997398376\n",
      "Predicted: [ 8 10 10 10 10 10 10]\n",
      "Sequence:  [ 8 10 10 10 10 10 10 10 10 10]\n",
      "Epoch 7 loss: 0.11628201603889465\n",
      "Predicted: [ 4  2  2  4  5  3 10]\n",
      "Sequence:  [ 4  2  2  4  5  3 10 10 10 10]\n",
      "Epoch 8 loss: 0.12288783490657806\n",
      "Predicted: [ 1 10 10 10 10 10 10 10 10]\n",
      "Sequence:  [ 1 10 10 10 10 10 10 10 10 10]\n",
      "Epoch 9 loss: 0.09691944718360901\n",
      "Predicted: [ 9  3  8  7  3  2  9  7 10]\n",
      "Sequence:  [ 9  3  8  7  3  2  9  7 10 10]\n",
      "Epoch 10 loss: 0.07563913613557816\n",
      "Predicted: [ 5  0  6 10 10 10 10 10 10]\n",
      "Sequence:  [ 5  0  6 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    for _ in range(100):\n",
    "        images, sequences = generate_batch(32)\n",
    "        sess.run([train_op], feed_dict={\n",
    "            inputs: images / 256.,\n",
    "            targets: sequences})\n",
    "\n",
    "    images, sequences = generate_batch(8)\n",
    "    loss, preds = sess.run([model_loss, model_preds],\n",
    "                           feed_dict={\n",
    "                               inputs: images / 256.,\n",
    "                               targets: sequences})\n",
    "    print('Epoch {} loss: {}'.format(e + 1, loss))\n",
    "    print('Predicted: {}'.format(preds[0]))\n",
    "    print('Sequence:  {}'.format(sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "1. Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks\n",
    "Graves, A., Fernandez, S., Gomez, F. and Schmidhuber, J., 2006 (ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf) <br>\n",
    "2. Sequence Modeling With CTC (https://distill.pub/2017/ctc/) <br>\n",
    "3. https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss_v2"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
